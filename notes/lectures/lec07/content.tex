\lecture{7}
\title{Inductive Definitions and Least Fixed Points}
\date{12 February 2016}
\maketitle

\section{Set Operators}

Last time we considered a set of \emph{rule instances} of the form
\begin{equation}
\frac{X_1\ X_2\ \ldots\ X_n}{X},\label{eqn:rule}
\end{equation}
where $X$ and the $X_i$ are members of some set $S$. From this we defined a set operator
\begin{align}
R(B) &\definedas \{X \mid \mbox{$\{X_1,X_2,\ldots,X_n\}\subseteq B$ and $\displaystyle\frac{X_1\ X_2\ \ldots\ X_n}X$ is a rule instance}\}.\label{eqn:rulebased}
\end{align}

An important property of set operators defined by rules of this form is \emph{monotonicity}. A set operator $R$ is \emph{monotone} if $B\subseteq C$ implies $R(B)\subseteq R(C)$. We listed two desirable properties of the set $A\subseteq S$ defined by $R$, namely:
\begin{itemize}
\item
$A$ should be \emph{$R$-closed}: $R(A)\subseteq A$. We would like this to hold because we would like every element that the rules say should be in $A$ to actually be in $A$.
\item
$A$ should be \emph{$R$-consistent}: $A\subseteq R(A)$. We would like this to hold because we would like every element of $A$ to be included in $A$ \emph{only} as a result of applying a rule.
\end{itemize}
These two properties together say that $A=R(A)$, or in other words, $A$ should be a \emph{fixed point} of $R$. We then asked two natural questions:
\begin{itemize}
\item
Does $R$ actually have a fixed point?
\item
Is the fixed point unique? If not, which one should we take?
\end{itemize}

\section{Least Fixed Points}

In fact, any monotone set operator $R$ always has at least one fixed point. It may have many, but among all its fixed points, it has a unique minimal one with respect to set inclusion $\subseteq$; that is, a fixed point that is a subset of all other fixed points of $R$.
We call this the \emph{least fixed point} of $R$.
It also has a \emph{greatest fixed point}, which is a fixed point that is a superset of all other fixed points of $R$.

The least fixed point of $R$ can be defined in two different ways, ``from below'' and ``from above''. For the rule-based operators we have been studying, these constructions take the following form:
\begin{align}
A_* &\definedas \bigcup\ \{R^n(\emptyset) \mid n\geq 0\}\ =\ R(\emptyset) \cup R(R(\emptyset)) \cup R(R(R(\emptyset)))\cup \cdots\label{eqn:KT1}\\
A^* &\definedas \bigcap\ \{B\subseteq S \mid R(B)\subseteq B\}.\label{eqn:KT2}
\end{align}
The set $A_*$ is the union of all sets of the form $R^n(\emptyset)$, the sets obtained by applying $R$ some finite number of times to the empty set.
The set $A^*$ is the intersection of all the $R$-closed subsets of $S$.
We will show that $A_* = A^*$ and that this set is the least fixed point of $R$.

\section{The Knaster--Tarski Theorem}

The fact that $A_* = A^*$ is a special case of a more general theorem called the \emph{Knaster--Tarski theorem}. It states that any monotone set operator $R$ has a unique least fixed point, and that this fixed point can be obtained either ``from below'' by iteratively applying $R$ to the empty set, or ``from above'' by taking the intersection of all $R$-closed sets.

For general monotone operators $R$, the ``from below'' construction may require iteration through transfinite ordinals. However, the operators $R$ defined from rule systems as described above are \emph{chain-continuous} (definition below). This is a stronger property than monotonicity. It guarantees that the ``from below'' construction converges to a fixed point after only $\omega$ steps, where $\omega$ is the first transfinite ordinal.

\subsection{Monotone, Continuous, and Finitary Operators}

If $\CC$ is a set of subsets of $S$, denote by $\bigcup\CC$ the union of all elements of $\CC$; that is, $x\in\bigcup\CC$ iff there exists $A\in\CC$ such that $x\in A$. The set $\bigcup\CC$ is the smallest subset of $S$ containing all $A\in\CC$ as subsets. A \emph{chain} is a set $\CC$ of subsets of $S$ that is linearly ordered by the set inclusion relation $\subseteq$; that is, for all $B,C\in\CC$, either $B\subseteq C$ or $C\subseteq B$.

A set operator $R:\powerset S\rightarrow\powerset S$ is said to be
\begin{itemize}
\item \emph{monotone} if $B\subseteq C$ implies $R(B)\subseteq R(C)$;
\item \emph{{\upshape(}chain-{\upshape)}continuous} if for any chain of sets $\CC$,
\begin{align*}
R(\,\bigcup\,\CC) &= \bigcup\ \{R(B) \mid B\in\CC\};
\end{align*}
\item \emph{finitary} if for any set $C$, the value of $R(C)$ is determined by the values of $R(B)$ for finite subsets $B\subseteq C$ in the following sense:
\begin{align*}
R(C) &= \bigcup\ \{R(B) \mid \mbox{$B\subseteq C$, $B$ finite}\}.
\end{align*}
\end{itemize}
One can show
\begin{enumerate}
\item
every rule-based operator of the form (\ref{eqn:rulebased}) is finitary;
\item
every finitary operator is chain-continuous (in fact, the converse holds as well);
\item
every chain-continuous operator is monotone.
\end{enumerate}
The proofs of 1, 2, and 3 are fairly straightforward and we will leave them as exercises. The converse of 2 requires transfinite induction and is more difficult.

\subsection{Proof of the Knaster--Tarski Theorem for Chain-Continuous Operators}

Let us prove the Knaster--Tarski theorem in the special case of chain-continuous operators, which will allow us to avoid introducing transfinite ordinals (not that they are not worth introducing!), and that is all we need to handle rule-based inductive definitions.

\medskip

\noindent
\textbf{Theorem (Knaster--Tarski)}\quad
Let $R:\powerset S\rightarrow\powerset S$ be a chain-continuous set operator, and let $A_*$ and $A^*$ be defined as in (\ref{eqn:KT1}) and (\ref{eqn:KT2}), respectively. Then $A_*=A^*$, and this set is the $\subseteq$-least fixed point of $R$.
\begin{proof}
The theorem follows from two observations:
\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
For every $n$ and every $R$-closed set $B$, $R^n(\emptyset)\subseteq B$. This can be proved by induction on $n$. It follows that $A_*\subseteq A^*$.
\item
$A_*$ is $R$-closed. Since $A^*$ is contained in all $R$-closed sets, $A^*\subseteq A_*$.
\end{enumerate}

For (i), let $B$ be an $R$-closed set. We proceed by induction on $n$. The basis for $n=0$ is $\emptyset\subseteq B$, which is trivially true. Now suppose $R^n(\emptyset)\subseteq B$. We have
\reasoning{R(R^n(\emptyset))}
\begin{align*}
R^{n+1}(\emptyset) &= R(R^n(\emptyset))\\
&\subseteq \reason{R(B)}{by the induction hypothesis and monotonicity}\\
&\subseteq \reason B{since $B$ is $R$-closed.}
\end{align*}
We conclude that for all $n$ and all $R$-closed sets $B$, $R^n(\emptyset)\subseteq B$, therefore
\begin{align*}
A_*\ &=\ \bigcup\ \{R^n(\emptyset) \mid n\geq 0\}\ \subseteq\ \bigcap\ \{B\subseteq S \mid R(B)\subseteq B\}\ =\ A^*.
\end{align*}

For (ii), we want to show that $R(A_*)\subseteq A_*$. It can be proved by induction on $n$ that the sets $R^n(\emptyset)$ form a chain:
\begin{align*}
\emptyset\ &\subseteq\ R(\emptyset)\ \subseteq\ R^2(\emptyset)\ \subseteq\ R^3(\emptyset)\ \subseteq\ \cdots
\end{align*}
We have $\emptyset\subseteq R(\emptyset)$ trivially, and by monotonicity, if $R^n(\emptyset)\subseteq R^{n+1}(\emptyset)$, then \begin{align*}
R^{n+1}(\emptyset)\ &=\ R(R^{n}(\emptyset))\ \subseteq\ R(R^{n+1}(\emptyset))\ =\ R^{n+2}(\emptyset).
\end{align*}
Now by chain-continuity,
\begin{align*}
R(A_*)\ &=\ R(\bigcup_{n\geq 0}\ R^n(\emptyset))\ =\ \bigcup_{n\geq 0}\ R(R^n(\emptyset))\ =\ \bigcup_{n\geq 0}\ R^{n+1}(\emptyset)\ =\ A_*.
\end{align*}
\end{proof}

\section{Rule Induction}

Let us use our newfound wisdom on well-founded induction and least fixed points of monotone maps to prove some properties of the reduction rules.

\subsection{Example: CBV Preserves Closedness}

\begin{theorem}
\label{thm:CBVpreservesclosedness}
If $e\rightarrow e'$ under the CBV reduction rules, then $\FV{e'}\subseteq\FV e$. In other words, CBV reductions cannot introduce any new free variables.
\end{theorem}

\begin{proof}
By induction on the CBV derivation of $e\rightarrow e'$. There is one case for each CBV rule, corresponding to each way $e\rightarrow e'$ could be derived.

\paragraph{Case 1}
$\displaystyle\frac{e_1\rightarrow e'_1}{e_1\,e_2\rightarrow e'_1\,e_2}$.

We assume that the desired property is true of the premise---this is the induction hypothesis---and we wish to prove under this assumption that it is true for the conclusion. Thus we are assuming that $\FV{e'_1}\subseteq\FV{e_1}$ and wish to prove that $\FV{e'_1\,e_2}\subseteq\FV{e_1\,e_2}$.
\reasoning{\FV{e'_1}\cup\FV{e_2}}
\begin{align*}
\FV{e'_1\,e_2} &= \reason{\FV{e'_1}\cup\FV{e_2}}{by the definition of \FVname}\\
&\subseteq \reason{\FV{e_1}\cup\FV{e_2}}{by the induction hypothesis}\\
&= \reason{\FV{e_1\,e_2}}{again by the definition of \FVname.}
\end{align*}

\paragraph{Case 2}
$\displaystyle\frac{e_2\rightarrow e'_2}{v\,e_2\rightarrow v\,e'_2}$.

This case is similar to Case 1, where now $e_2\rightarrow e'_2$ is used in the induction hypothesis.

\paragraph{Case 3}
$\displaystyle\frac{}{(\lam xe)\,v\rightarrow\subst evx}$.

There is no induction hypothesis for this case, since there is no premise in the rule; thus this case constitutes the basis of our induction. We wish to show, independently of any inductive assumption, that $\FV{\subst evx}\subseteq\FV{(\lam xe)\,v}$.

This case requires a lemma, stated below, to show that $\FV{\subst evx} \subseteq (\FV e - \{x\}) \cup \FV v$. Once that is shown, we have
\reasoning{(\FV e - \{x\})\cup\FV v}
\begin{align*}
\FV{\subst evx} &\subseteq \reason{(\FV e - \{x\})\cup\FV v}{by the lemma to be proved}\\
&= \reason{\FV{\lam xe} \cup \FV v}{definition of \FVname}\\
&= \reason{\FV{(\lam xe)\,v}}{definition of \FVname.}
\end{align*}

We have now considered all three rules of derivation for the CBV $\lambda$-calculus, so the theorem is proved.
\end{proof}

The following lemma is used in Case 3 of Theorem \ref{thm:CBVpreservesclosedness} above.

\begin{lemma}
$\FV{\subst evx} \subseteq (\FV e - \{x\}) \cup \FV v$.
\end{lemma}

\begin{proof}
By structural induction on $e$. There is one case for each clause in the definition of the substitution operator. We have assumed previously that values are closed terms, so $\FV v=\emptyset$ for any value $v$; but actually we do not need this for the proof, and we do not assume it.

\paragraph{Case 1}
$e = x$.
\reasoning{(\FV x - \{x\}) \cup \FV v}
\begin{align*}
\FV{\subst xvx}
&= \reason{\FV v}{definition of substitution}\\
&= (\{x\} - \{x\}) \cup \FV v\\
&= \reason{(\FV x - \{x\}) \cup \FV v}{definition of \FVname.}
\end{align*}

\paragraph{Case 2}
$e = y$, $y\neq x$.
\reasoning{(\FV y - \{x\}) \cup \FV v}
\begin{align*}
\FV{\subst yvx}
&= \reason{\FV y}{definition of substitution}\\
&= \reason{\{y\}}{definition of \FVname}\\
&\subseteq (\{y\} - \{x\}) \cup \FV v\\
&= \reason{(\FV y - \{x\}) \cup \FV v}{definition of \FVname.}
\end{align*}

\paragraph{Case 3}
$e = e_1\,e_2$.
\reasoning{(\FV{e_1} - \{x\}) \cup \FV v \cup (\FV{e_2} - \{x\}) \cup \FV v}
\begin{align*}
\FV{\subst{(e_1\,e_2)}vx}
&= \reason{\FV{\subst{e_1}vx\,\subst{e_2}vx}}{definition of substitution}\\
&\subseteq \reason{(\FV{e_1} - \{x\}) \cup \FV v \cup (\FV{e_2} - \{x\}) \cup \FV v}{induction hypothesis}\\
&= ((\FV{e_1} \cup \FV{e_2}) - \{x\}) \cup \FV v\\
&= \reason{(\FV{e_1\,e_2} - \{x\}) \cup \FV v}{definition of \FVname.}
\end{align*}

\paragraph{Case 4}
$e = \lam x{e'}$.
\reasoning{(\FV{\lam x{e'}} - \{x\}) \cup \FV v}
\begin{align*}
\FV{\subst{(\lam x{e'})}vx}
&= \reason{\FV{\lam x{e'}}}{definition of substitution}\\
&= \reason{\FV{\lam x{e'}} - \{x\}}{because $x\not\in\FV{\lam x{e'}}$}\\
&\subseteq (\FV{\lam x{e'}} - \{x\}) \cup \FV v.
\end{align*}

\paragraph{Case 5}
$e = \lam y{e'}$, $y \neq x$. This is the most interesting case, because it involves a change of bound variable. Using the fact $\FV v = \emptyset$ for values $v$ would give a slightly simpler proof. Let $v$ be a value and $z$ a variable such that $z\neq x$, $z\not\in\FV{e'}$, and $z\not\in\FV v$.
\reasoning{((((\FV{e'} - \{y\}) \cup \FV z) - \{x\}) \cup \FV v) - \{z\}}
\begin{align*}
\FV{\subst{(\lam y{e'})}vx}
&= \reason{\FV{\lam z{\subst{\subst{e'}zy}vx}}}{definition of substitution}\\
&= \reason{\FV{\subst{\subst{e'}zy}vx} - \{z\}}{definition of \FVname}\\
&= \reason{((((\FV{e'} - \{y\}) \cup \FV z) - \{x\}) \cup \FV v) - \{z\}}{induction hypothesis (twice)}\\
&= \reason{(((\FV{\lam y{e'}} \cup \{z\}) - \{x\}) \cup \FV v) - \{z\}}{definition of \FVname}\\
&= ((\FV{\lam y{e'}} - \{x\}) \cup \FV v \cup \{z\}) - \{z\}\\
&= (\FV{\lam y{e'}} - \{x\}) \cup \FV v.
\end{align*}
\end{proof}

There is a subtle point that arises in Case 5. We said at the beginning of the proof that we would be doing structural induction on $e$; that is, induction on the well-founded subterm relation $\lt$. This was a lie. Because of the change of bound variable necessary in Case 5, we are actually doing induction on the relation of subterm modulo $\alpha$-equivalence:
\begin{align*}
e \lt_\alpha e' &\definedas \exists e''\ e'' \lt e' \wedge e=_\alpha e''.
\end{align*}
But a moment's thought reveals that this relation is still well-founded, since $\alpha$-reduction does not change the size or shape of the term, so we are ok.

\iffalse

\subsection{Example: Agreement of Big-Step and Small-Step Semantics}

We can express the idea that the two semantics should
agree on terminating executions by connecting the relations $\rightarrow$ and $\stepsto$:
\begin{align}
\config a\sigma \rightarrow_a \overline n\ &\Iff\ \config a\sigma\stepsto_a n\label{eqn:golda}\\
\config b\sigma \rightarrow_b \overline t\ &\Iff\ \config b\sigma\stepsto_b t\label{eqn:goldb}\\
\config c\sigma \rightarrow \config\SKIP{\sigma'}\ &\Iff\ \config c\sigma\stepsto \sigma',\label{eqn:goldc}
\end{align}
where $\overline\false=\FALSE$ and $\overline\true=\TRUE$.
These can all be proved using induction. We can prove \eqref{eqn:golda} and \eqref{eqn:goldb} as lemmas
separately, then use these lemmas to prove \eqref{eqn:goldc}.

Let us just show a few of the cases for \eqref{eqn:goldc}.
To prove the forward implication of \eqref{eqn:goldc},
we can use structural induction on $c$. The converse requires
induction on the derivation of the big-step evaluation.

Suppose we are given $\config c\sigma\stepsto\sigma'$. The
form of the derivation $\config c\sigma\stepsto\sigma'$ depends on the form of $c$.

\begin{itemize}
\item
Case $\SKIP$. In this case we know $\sigma = \sigma'$, and trivially,
$\config\SKIP\sigma \rightarrow \config\SKIP\sigma$.
\item
Case $\assg xa$. In this case we know from the premises that
$\config a\sigma \stepsto n$ for some $n$ and that $\sigma' = \rebind\sigma nx$.

We will need a lemma to the effect that $\config a\sigma\rightarrow_a\overline n$ implies that $\config{\assg xa}\sigma
\rightarrow \config{\assg x{\overline n}}\sigma$. This result can be proved easily using an
induction on the number of steps in the derivation $\config a\sigma\rightarrow_a\overline n$.
Given this and \eqref{eqn:golda}, we have that $\config{\assg xa}\sigma \rightarrow \config{\assg x{\overline n}}\sigma$ and
$\config{\assg x{\overline n}}\sigma \stepsone \config\SKIP{\rebind\sigma nx}$, so
$\config{\assg xa}\sigma \rightarrow \config\SKIP{\rebind\sigma nx}$.

\item
Case $\whiledo bc$, where $\config b\sigma \stepsto_b \false$. Then $\sigma=\sigma'$.
In small-step semantics, we have an initial step
\begin{align}
\config{\whiledo bc}\sigma\ &\stepsone\ \config{\ifthenelse b{(\comp c{\whiledo bc})}\SKIP}\sigma.\label{eqn:wh}
\end{align}
By \eqref{eqn:goldb},
$\config b\sigma \rightarrow_b \FALSE$. We need another lemma to the effect that if
$\config b\sigma \rightarrow_b t$, then
\begin{align}
\config{\ifthenelse bc{c'}}\sigma\ &\rightarrow\ \config{\ifthenelse tc{c'}}\sigma.\label{eqn:wh2}
\end{align}
In this case $t=\FALSE$, thus \eqref{eqn:wh} becomes $\config\SKIP\sigma$ as desired.

\item
Case $\whiledo bc$, where $\config b\sigma \stepsto_b \true$.
This is the most interesting case in the entire proof.
In small-step semantics, we have the initial step \eqref{eqn:wh}, as in the previous case.
From \eqref{eqn:wh2} with $t=\TRUE$, \eqref{eqn:wh} becomes
$\config{\comp c{\whiledo bc}}\sigma$. We need one more lemma for stitching together
small-step executions:
\begin{align}
\config{c_1}\sigma \rightarrow \config\SKIP{\sigma''}\ \wedge\ \config{c_2}{\sigma''} \rightarrow \config\SKIP{\sigma'}\
\Imp\ \config{\comp{c_1}{c_2}}\sigma \rightarrow \config\SKIP{\sigma'}. \label{eqn:seq}
\end{align}

This can be proved by induction on the number of steps.

Now, because
$\config{\whiledo bc}\sigma \stepsto \sigma'$ and $\config b\sigma \stepsto_b \true$, we know that
$\config c\sigma \stepsto \sigma''$ and $\config{\whiledo bc}{\sigma''} \stepsto \sigma'$ for some $\sigma''$. Furthermore,
because these two derivations are subderivations
of the derivation $\config{\whiledo bc}\sigma\stepsto\sigma'$, the induction hypothesis gives us
$\config c\sigma \rightarrow \config\SKIP{\sigma''}$ and
$\config{\whiledo bc}{\sigma''}\rightarrow \sigma'$. Using \eqref{eqn:seq},
we have $\config{\comp c{\whiledo bc}}\sigma \rightarrow \config\SKIP{\sigma'}$.

We could not have used structural induction for this proof,
because the induction step involved relating an evaluation of the
command $\whiledo bc$ to a different evaluation of the same command
rather than to an evaluation of a subexpression.
\end{itemize}

\fi

\section{Remark}

These proofs may seem rather tedious, and one may wonder why we are doing them in such detail. But of course, this is exactly the point. Formal reasoning about the semantics of the $\lambda$-calculus, including such seemingly complicated notions as reductions and substitutions, can be reduced to the mindless application of a few simple rules. There is no hand-waving, no magic, no hidden behavior. To the extent that we can do this for real programming languages, we will be better able to understand and predict their behavior and to automate parts of the reasoning process.
