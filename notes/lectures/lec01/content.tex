\lecture{1}
\title{Introduction}
\maketitle

\section{Introduction}

What is a program? Is it just something that tells the
computer what to do? Yes, but there is much more to it than that.
The basic expressions in a program must be interpreted somehow,
and a program's behavior depends on how they are interpreted. We must have
a good understanding of this interpretation, otherwise it would be
impossible to write programs that do what is intended.

It may seem like a straightforward task to specify what a program is supposed
to do when it executes. After all, basic instructions are pretty simple.
But in fact this task is often quite subtle and difficult. Programming
language features often interact in ways that are unexpected or hard to predict.
Ideally it would seem desirable to be able to determine the meaning of a program
completely by the program text, but that is not always true, as you well
know if you have ever tried to port a C program from one platform to another.
Even for languages that are nominally platform-independent, meaning is not
necessarily determined by program text. For example, consider the following Java fragment.

\begin{flushleft}
\texttt{\phantom{dddd}class A \{ static int a = B.b + 1; \}}\\
\texttt{\phantom{dddd}class B \{ static int b = A.a + 1; \}}
\end{flushleft}

First of all, is this even legal Java? Yes, although no sane programmer
would ever write it. So what happens when the classes are initialized?
A reasonable educated guess might be that the program goes into an
infinite loop trying to initialize \texttt{A.a} and \texttt{B.b} from each other. But no,
the initialization terminates with initial values for \texttt{A.a} and \texttt{B.b}.
So what are the initial values? Try it and find out,
you may be surprised. Can you explain what is going on?
This simple bit of pathology illustrates the difficulties that can arise
in describing the meaning of programs. Luckily, these are the exception, not the rule.

Programs describe computation, but they are more than just lists of instructions.
They are mathematical objects as well. A programming language is a logical formalism,
just like first-order logic. Such formalisms typically consist of
\begin{itemize}
\item
\emph{Syntax}, a strict set of rules telling how to distinguish well-formed expressions from arbitrary sequences of symbols; and
\item
\emph{Semantics}, a way of interpreting the well-formed expressions. The word ``semantics'' is a synonym for ``meaning'' or ``interpretation''. Although ostensibly plural, it customarily takes a singular verb.
Semantics may include a notion of \emph{deduction} or \emph{computation}, which determines how the system performs work. 
\end{itemize}

In this course we will see some of the formal tools that have been developed
for describing these notions precisely. The course consists of three major components:
\begin{itemize}
\item
\emph{Dynamic semantics}---methods for describing and reasoning about
what happens when a program runs.
\item
\emph{Static semantics}---methods for reasoning about
programs _before_ they run. Such methods include type checking, type
inference, and static analysis. We would like to find errors in
programs as early as possible. By doing so, we can often detect
errors that would otherwise show up only at runtime, perhaps after
significant damage has already been done.
\item
\emph{Language features}---applying the tools of dynamic and static semantics to study
actual language features of interest, including some that you may not have encountered previously.
\end{itemize}
We want to be as precise as possible about these notions.
Ideally, the more precisely we can describe the semantics of a programming language,
the better equipped we will be to understand its power and limitations and 
to predict its behavior. Such understanding is essential not only for
writing correct programs, but also for building tools like compilers,
optimizers, and interpreters. Understanding the meaning of programs
allows us to ascertain whether these tools are implemented correctly.
But the very notion of \emph{correctness} is subject to semantic interpretation.

It should be clear that the task before us is inherently mathematical.
Initially, we will characterize the semantics of a program as a function
that produces an output value based on some input value.
Thus, we will start by presenting some mathematical
tools for constructing and reasoning about functions.

\subsection{Binary Relations and Functions}

Denote by $A\times B$ the set of all ordered pairs $(a,b)$ with $a\in A$ and $b\in B$.
A _binary relation_ on $A\times B$ is just a subset $R\subseteq A\times B$.
The sets $A$ and $B$ can be the same, but they do not have to be. The set $A$
is called the _domain_ and $B$ the _codomain_ (or _range_) of $R$. The smallest binary
relation on $A\times B$ is the null relation $\emptyset$ consisting
of no pairs, and the largest binary relation on $A\times B$ is $A\times B$ itself.
The _identity relation_ on $A$ is $\set{(a,a)}{a\in A}\subseteq A\times A$.

An important operation on binary relations is _relational composition_
\[
\comp RS = \set{(a,c)}{\exists b\ (a,b)\in R \wedge (b,c)\in S},
\]
where the codomain of $R$ is the same as the domain of $S$.

A (_total_) _function_ (or _map_) is a binary relation $f\subseteq A\times B$ in which each element of $A$
is associated with exactly one element of $B$. If $f$ is such a function, we write:
\[
f : A \to B
\]
In other words, a function $f:A\to B$ is a binary relation $f\subseteq A\times B$ such that for each
element $a\in A$, there is exactly one pair $(a,b)\in f$ with first component $a$.
There can be more than one element of $A$ associated with the same element of $B$, and not all elements
of $B$ need be associated with an element of $A$. 

The set $A$ is the _domain_ and $B$ is the
_codomain_ or _range_ of $f$. The _image_ of $f$ is the set of elements in
$B$ that come from at least one element in $A$ under $f$:
\begin{eqnarray*}
f(A) &\definedas& \set{x\in B}{x = f(a) \text{ for some } a\in A}\\
&=& \set{f(a)}{a\in A}.
\end{eqnarray*}
The notation $f(A)$ is standard, albeit somewhat of an abuse.

The operation of \emph{functional composition} is: if $f:A\to B$ and $g:B\to C$, then $g\circ f:A\to C$ is the function
\[
(g\circ f)(x) = g(f(x)).
\]
Viewing functions as a special case of binary relations, functional composition is the same as relational composition, but the order is reversed in the notation: $g\circ f=\comp fg$.

A _partial function_ $f: A\rightharpoonup B$ (note the shape of the arrow) is a function $f:A'\to B$ defined on some subset $A'\subseteq A$. The notation $\dom f$ refers to $A'$, the domain of $f$. If $f:A\to B$ is total, then $\dom f = A$.

A function $f:A\rightarrow B$ is said to be _one-to-one_ (or _injective_) if $a\neq b$ implies $f(a)\neq f(b)$ and _onto_ (or _surjective_) if every $b\in B$ is $f(a)$ for some $a\in A$.

\subsection{Representation of Functions}

Mathematically, a function is equal to its _extension_, which is the set of all its (input, output) pairs. One way to describe a function is to describe its extension directly, usually by specifying some mathematical relationship between the inputs and outputs. This is called an _extensional_ representation. Another way is to give an
_intensional_\footnote{Note the spelling: _intensional_ and _intentional_ are not the same!}
representation, which is essentially a program or evaluation procedure to compute the output corresponding to a given input. The main differences are
\begin{itemize}
\item
there can be more than one intensional representation of the same function, but there is only one extension;
\item
intensional representations typically give a method for computing the output from a given input, whereas extensional representations need not concern themselves with computation (and seldom do).
\end{itemize}

A central issue in semantics---and a good part of this course---is concerned with how to go from an intensional representation to a corresponding extensional representation.

\section{The $\lambda$-Calculus}

The $\lambda$-calculus ($\lambda$ = ``lambda'', the Greek letter l)\footnote{Why $\lambda$? To distinguish the bound
variables from the unbound (free) variables, Church placed a caret on top of
the bound variables, thus $\lam x{x + yx^2}$ was represented as
$\hat x\,.\,x + yx^2$. Apparently, the printers could not handle the caret,
so it moved to the front and became a $\lambda$.}
was introduced by Alonzo Church (1903--1995) and Stephen Cole Kleene (1909--1994) in the 1930s to study
the interaction of \emph{functional abstraction} and \emph{functional application}.
The $\lambda$-calculus provides a succinct and unambiguous notation
for the _intensional_ representation of functions, as well as a
general mechanism based on substitution for evaluating them.

The $\lambda$-calculus forms the theoretical foundation of all modern functional programming languages,
including Lisp, Scheme, Haskell, OCaml, and Standard ML. One cannot understand the semantics of these
languages without a thorough understanding of the $\lambda$-calculus.

It is common to use $\lambda$-notation in conjunction with other operators and
values in some domain (e.g.~$\lam x {x+2}$), but the _pure_ $\lambda$-calculus has
only $\lambda$-terms and only the operators of functional abstraction and functional application,
nothing else. In the pure $\lambda$-calculus, $\lambda$-terms act as functions that
take other $\lambda$-terms as input and produce $\lambda$-terms as output.
Nevertheless, it is possible to code common data structures such as Booleans, integers,
lists, and trees as $\lambda$-terms. The $\lambda$-calculus is computationally powerful
enough to represent and compute any computable function over these data structures.
It is thus equivalent to Turing machines in computational power.

\subsection{Syntax}

The following is the syntax of the \emph{pure} \emph{untyped} $\lambda$-calculus. Here \emph{pure} means there
are no constructs other than $\lambda$-terms, and \emph{untyped} means that there are no restrictions on how
$\lambda$-terms can be combined to form other $\lambda$-terms; every well-formed $\lambda$-term is considered
meaningful.

A $\lambda$-term is defined inductively as follows. Let \Var\ be a countably infinite set of variables $x,y,\ldots$~.
\begin{itemize}
\item
Any variable $x\in\Var$ is a $\lambda$-term.
\item
If $e$ is a $\lambda$-term, then so is $\lam x e$ (functional abstraction).
\item
If $e_1$ and $e_2$ are $\lambda$-terms, then so is $e_1\cdot e_2$ (functional application).
\end{itemize}
We usually omit the application operator $\cdot$, writing $e_1\,(e_2)$, $(e_1\,e_2)$, or even $e_1\,e_2$ for $e_1\cdot e_2$. Intuitively, this term represents the result of applying of $e_1$ as a function to $e_2$ as its input. The term $\lam x e$ represents a function with input parameter $x$ and body $e$.

\subsection{Examples}

A term representing the identity function is $\ID = \lam xx$.
The term $\lam x {\lam aa}$ represents a function that ignores its argument and return the identity function.
This is the same as $\lam x\ID.$

The term $\lam f{fa}$ represents a function that takes another function $f$ as
an argument and applies it to $a$. Thus we can define functions that can take other functions
as arguments and return functions as results; that is, functions are _first-class values_.
The term $\lam v{\lam f{fv}}$
represents a function that takes an argument $v$ and returns a function $\lam f{fv}$
that calls its argument---some function $f$---on $v$. A function that takes a pair of functions $f$ and $g$ 
and returns their composition $g\circ f$ is represented by $\lam f{\lam g{\lam x{g(fx)}}}$.
We could \emph{define} the composition operator this way.

In the pure $\lambda$-calculus, every $\lambda$-term represents a function, since any $\lambda$-term
can appear on the left-hand side of an application operator. 

\subsection{BNF Notation}

\emph{Backus--Naur form} (BNF) is a kind of grammar used to specify the
syntax of programming languages. It is named for
John Backus (1924--2007), the inventor of Fortran, and Peter Naur (1928--),
the inventor of Algol 60.

We can express the syntax of the pure untyped $\lambda$-calculus very concisely
using BNF notation:
\[
e\ \ ::=\ \ x \bnf e_1\,e_2 \bnf \lam xe
\]
Here the $e$ is a _metavariable_ representing a \emph{syntactic class}
(in this case $\lambda$-terms) in the language. It is not a variable at the level
of the programming language. We use subscripts to differentiate
metavariables of the same syntactic class. In this definition, $e_0$, $e_1$ and
$e$ all represent $\lambda$-terms.

The pure untyped $\lambda$-calculus has only
two syntactic classes, variables and $\lambda$-terms, but we shall soon
see other more complicated BNF definitions.

\subsection{Other Domains}

The $\lambda$-calculus can be used in conjunction with other domains of primitive values and operations on them. Some popular choices are the \emph{natural numbers} $\naturals = \{0,1,2,\ldots\}$ and \emph{integers} $\integers = \{\ldots,-2,-1,0,1,2,\ldots\}$ along with the basic arithmetic operations $+,\cdot$ and tests $=,\leq,\lt$; and the two-element Boolean algebra $\Two = \{\false,\true\}$ along with the basic Boolean operations $\wedge$ (and), $\vee$ (or), $\neg$ (not), and $\imp$ (implication).\footnote{The German mathematician Leopold Kronecker (1823--1891) was fond of saying, ``God created the natural numbers; all else is the work of Man.'' Actually, there is not much evidence that God created $\naturals$. But for $\Two$, there is no question:
\begin{quote}
\emph{And the earth was without form, and void\ldots And God said, Let there be light\ldots And God divided the light from the darkness\ldots} \quad ---Genesis 1\,:\,2--4
\end{quote}}

The $\lambda$-calculus gives a convenient notation for describing functions built from these objects.
We can incorporate them in the language by assuming there is a constant for each primitive value and each distinguished operation, and extending the definition of \emph{term} accordingly. This allows us to write expressions like
$\lam x{x^2}$ for the squaring function on integers.

In mathematics, it is common to define a function $f$ by describing its value $f(x)$ on a typical input $x$. For example, one might specify the squaring function on integers by writing $f(x)=x^2$, or anonymously by $x\mapsto x^2$. Using $\lambda$-notation, we would write $f=\lam x{x^2}$ or $\lam x{x^2}$, respectively.

\subsection{Abstract Syntax and Parsing Conventions}

The BNF definition above actually defines the \emph{abstract syntax} of the
$\lambda$-calculus; that is, we consider a term to be already parsed into
its \emph{abstract syntax tree}.
In the text, however, we are constrained to use sequences of symbols to represent
terms, so we need some conventions to be sure that they are read unambiguously.

We use parentheses to show explicitly how to parse expressions, but we
also assign a precedence to the operators in order to save parentheses.
Conventionally, function application is higher precedence (binds tighter) than $\lambda$-abstraction;
thus $\lam xx\,\lam yy$ should be read as $\lam x{(x\,\lam yy)}$, not
$(\lam xx)\,(\lam yy)$. If you want the latter, you must use explicit parentheses.

Another way to view this is that the body of a $\lambda$-abstraction $\lam x\ldots$ extends as
far to the right as it can---it is \emph{greedy}. Thus the body
is delimited on the right only by a right parenthesis whose matching
left parenthesis is to the left of the $\lambda x$, or by the end of the entire expression.

Another convention is that application is _left-associative_, which
means that $e_1\,e_2\,e_3$ should be read as $(e_1\,e_2)\,e_3$.
If you want $e_1\,(e_2\,e_3)$, you must use parentheses. 

It never hurts to include parentheses if you are not sure. 

\subsection{Terms and Types}

Typically, programming languages have two different kinds of expressions:
_terms_ and _types_. We have not talked about types yet, but we will soon.
A \emph{term} is an expression representing a value;
a \emph{type} is an expression representing a class of similar values.

The value represented by a term is determined at runtime by evaluating the term; its value at
compile time may not be known. Indeed, it may not even have a value if the evaluation does not
terminate. On the other hand, types can be determined at compile time and are used by the compiler to rule
out ill-formed terms. When we say a given term has a given type (for example, $\lambda x.x^2$ has type
$\integers\to\integers$), we are saying that the value of the term after evaluation at runtime,
if it exists, will be a member of the class of similar values represented by the type.

In the pure untyped $\lambda$-calculus, there are no types, and all terms are meaningful.

\subsection{Multi-Argument Functions and Currying}

We would like to allow multiple arguments to a function, as for example in $(\lam{(x,y)}{x+y})\,(5,2)$. However, we do not need to introduce a primitive pairing operator to do this. Instead, we can write $(\lam x{\lam y{x+y}})\,5\,2$. That is, instead of the function taking two arguments and adding them, the function takes only the first argument and returns a function that takes the second argument and then adds the two arguments. The notation $\lam{x_1\ldots x_n}e$ is considered an abbreviation for $\lam{x_1}\lam{x_2}\lam{x_3}\ldots\lam{x_n}e$. Thus we consider the multi-argument version of the $\lambda$-calculus as just syntactic sugar. The ``desugaring'' transformation
\begin{eqnarray*}
\lam{x_1\ldots x_n}e & \Rightarrow & \lam{x_1}{\lam{x_2}{\lam{x_n}e}}\\
e_0\,(e_1,\ldots,e_n) & \Rightarrow & e_0\,e_1\,e_2\,\cdots\,e_n
\end{eqnarray*}
for this particular form of sugar is called _currying_ after Haskell B.~Curry (1900--1982).

\section{Preview}

Next time we will discuss capture-avoiding (safe) substitution and the computational rules of the $\lambda$-calculus, namely $\alpha$-, $\beta$-, and $\eta$-reduction. This is the _calculus_ part of the $\lambda$-calculus.

\nocite{Barendregt84}
