\lecture{4}
\title{Reduction Strategies and Equivalence}
\date{3 February 2016}
\maketitle

\section{Reduction Strategies}

In general there may be many possible $\beta$-reductions that can be
performed on a given $\lambda$-term. How do we choose which one to
perform next? Does it matter?

A specification that tells which of the possible $\beta$-reductions to
perform next is called a _reduction strategy_.  The $\lambda$-calculus
does not specify a reduction strategy; it
is \emph{nondeterministic}. A reduction strategy is needed in real
programming languages to resolve the nondeterminism.

Two common reduction strategies for the $\lambda$-calculus
are \emph{normal order} and \emph{applicative order}. Under the normal
order reduction strategy, the leftmost-outermost redex is always the
next to be reduced. By leftmost-outermost, we mean that if $e_1$ and
$e_2$ are redexes in a term and $e_1$ is a subterm of $e_2$, then
$e_1$ will not be reduced next; and among those redexes that are not
subterms of other redexes, which are all pairwise incomparable with
respect to the subterm relation, the leftmost one is chosen for
reduction. It is known that if a term has a normal form at all, then
normal order reduction will converge to it.

The applicative order reduction strategy is similar, except that the
leftmost-innermost redex is chosen. That is, if $e_1$ and $e_2$ are
redexes in a term and $e_1$ is a subterm of $e_2$, then $e_2$ will not
be reduced next; and among those redexes that do not contain other
redexes as subterms, which are all pairwise incomparable with respect
to the subterm relation, the leftmost one is chosen for reduction.

In real functional programming languages, reductions inside the body
of a $\lambda$-abstraction are usually not performed (although
optimizing compilers may do so in some instances). If we restrict the
normal order and applicative order strategies so as not to perform
reductions inside the body of $\lambda$-abstractions, we obtain
strategies known as \emph{call-by-name} (CBN) and \emph{call-by-value}
(CBV), respectively.

Most functional programming languages use CBV, with the notable
exception of Haskell. Let us define a _value_ to be a $\lambda$-term
for which no $\beta$-reductions are possible, given our chosen
reduction strategy. For example, $\lam xx$ would always be a value,
whereas $(\lam xx)\,1$ would most likely not be.

Under CBV, functions may only be called on values; that is, the
arguments must be fully evaluated. Thus the $\beta$-reduction step
$(\lam x{e_1})\,e_2 \stepsone \subst{e_1}{e_2}{x}$ only applies if
$e_2$ is a value. Here is an example of a CBV evaluation sequence,
where we consider $3$ and $\SUCC$ (the successor function) to be
primitive constants.
\[
(\lam x{\SUCC\,x})\,((\lam y{\SUCC\,y})\,3)
\ \ \stepsone\ \ (\lam x{\SUCC\,x})\,(\SUCC\,3)
\ \ \stepsone\ \ (\lam x{\SUCC\,x})\,4
\ \ \stepsone\ \ \SUCC\,4
\ \ \stepsone\ \ 5.
\]
An alternative strategy is CBN. Under CBN, we defer evaluation of
arguments until as late as possible, applying reductions from left to
right within the expression. Here is the same term evaluated under
CBN.
\[
(\lam x{\SUCC\,x})\,((\lam y{\SUCC\,y})\,3)
\ \ \stepsone\ \ \SUCC\,((\lam y{\SUCC\,y})\,3)
\ \ \stepsone\ \ \SUCC\,(\SUCC\,3)
\ \ \stepsone\ \ \SUCC\,4
\ \ \stepsone\ \ 5.
\]
This is the preferred strategy of the language Haskell. Another way to
view this is as a form of \emph{lazy evaluation}; the arguments to a
function are not evaluated until they are actually needed.

\section{Structured Operational Semantics (SOS)}

Let's formalize CBV for the pure $\lambda$-calculus. First, we
restrict our attention to closed $\lambda$-terms.  Then the values of
the language are simply the closed $\lambda$-abstractions:
\[
v\ \ ::=\ \ \lam xe
\]
The use of this BNF definition specifies that the metavariable $v$
stands for a value; in this case, a closed $\lambda$-abstraction.

Next, we can write \emph{inference rules} to specify when reductions
are allowed:
\begin{equation}
\Rule{\phantom{e_1 \stepsone e'_1}}{(\lam xe)\,v \stepsone \subst evx}
\qquad
\Rule{e_1 \stepsone e'_1}{e_1\,e_2 \stepsone e'_1\,e_2}
\qquad
\Rule{e \stepsone e'}{ v\,e \stepsone v\,e'}
\label{eqn:cbvrules}
\end{equation}
This is a simple operational semantics for a programming language
based on the $\lambda$-calculus. An operational semantics is a
language semantics that describes how to run the program. This can be
done through informal human-language text, as in the Java Language
Specification \cite{Gosling05}, or through more formal rules, as we
have done here.

The leftmost rule of \eqref{eqn:cbvrules} is just
$\beta$-reduction. But by the use of the metavariable $v$ for the
argument of the function, we have indicated that the rule may only be
applied when the argument is a value. The second rule says that
$e_1\,e_2$ reduces to $e_1'\,e_2$ in one step provided $e_1$ reduces
to $e_1'$ in one step. The rightmost rule says that $v\,e$ reduces to
$v\,e'$ in one step provided $e$ reduces to $e'$ in one step and $v$
is already reduced.

Rules of the form \eqref{eqn:cbvrules} are known as a Structural
Operational Semantics (SOS). They define evaluation as the result of
applying the rules to transform the expression. The rules are
typically inductive on the structure of the expression being
evaluated.

As defined above, CBV evaluation is _deterministic_: there is at most
one evaluation rule that applies in any situation (we will prove this
later).

This kind of operational semantics is known as a _small-step_
semantics because it describes only one step at a time. An alternative
is a _big-step_ (or _large-step_) semantics that describes the entire
evaluation of the program to a final value.

We will see other kinds of semantics later in the course, such as
_axiomatic semantics_, which describes the behavior of a program in
terms of the observable properties of the input and output states, and
_denotational semantics_, which translates a program into an
underlying mathematical representation.

CBN has slightly simpler rules:
\[
\Rule{\phantom{e_0 \stepsone e'_0}}{(\lam x{e_1})\,e_2 \stepsone \subst{e_1}{e_2}{x}}
\qquad
\Rule{e_0 \stepsone e'_0}{e_0\,e_1 \stepsone e'_0\,e_1}
\]
We don't need the rule for evaluating the right-hand side of an
application because $\beta$-reductions are performed immediately once
the left-hand side is a value.

What happens if we try using $\Omega$ as a parameter? It depends on
the evaluation strategy. Consider
\[
(\lam x{\lam yy})\,\Omega
\]
Using the CBV evaluation strategy, we must first reduce $\Omega$. This
puts the evaluator into an infinite loop. On the other hand, CBN
reduces the term above to $\lam{y}{y}$. CBN has an important property:
CBN will not loop infinitely unless every other semantics would also
loop infinitely, yet it agrees with CBV whenever CBV terminates
successfully.

\subsection{Other Reduction Strategies}

As mentioned above, in _normal order_, the leftmost-outermost redex is
reduced first.  This is closely related to CBN evaluation, but also
allows reductions in the body of a $\lambda$-term.  Like CBN, it finds
a value if one exists, albeit not necessarily in the most efficient
way.  Call-by-value (CBV) is correspondingly related to _applicative
order_, where the argument to a function must be reduced to a value
before the function is applied.

In the programming language C, the order of evaluation of arguments is
not defined by the language; it is implementation-specific. Because of
this and the fact that C has side effects, C is not confluent. For
example, the value of the expression $(x=1) + x$ is 2 if the left
operand of $+$ is evaluated first, $x+1$ if the right operand is
evaluated first. This makes writing correct C programs more
challenging!

The absence of confluence in concurrent imperative languages is one
reason that concurrent programming is difficult. In the
$\lambda$-calculus, confluence guarantees that reductions can be done
in parallel without fear of changing the result.

%%%%%%%%%%%%%%%%%%%%%%%%

\section{Term Equivalence}

When should two terms be considered equal? This question is not as
simple as it may seem. The strictest definition of equality is
syntactic identity, but this is not very interesting or useful. For
example, it seems clear that $\lam xx$ and $\lam yy$ should be
considered equal, as the parameter name is inconsequential. So we
might declare two terms equal if they are syntactically identical
modulo $\alpha$-renaming. This is a reasonable definition if we wish
to regard $\lambda$-terms as _intensional_ objects.

As _extensional_ objects, however, it does not go far enough. We would
like to consider two terms equal if they represent the same
function. The terms $\lam xx$ and $\lam yy$ certainly represent the
same function (the identity), but there are others; for example, $\lam
x{(\lam yy)}\,x$. So terms do not have to be $\alpha$-equivalent to
represent the same function.  However, note that $\lam x{(\lam
yy)}\,x$ reduces to $\lam xx$ in one $\beta$-reduction step applied
inside the body of the outer $\lambda$-expression. So we might declare
two terms equal if either (i) they have a common normal form up to
$\alpha$-equivalence, or (ii) neither has a normal form; that is,
either they both converge to $\alpha$-equivalent values under some
sequence of reductions, or neither converges under any sequence of
reductions. By confluence, this is an equivalence relation. This
_normalization_ approach is useful for compiler optimization and for
checking type equality in some advanced type systems. Unfortunately,
it would not work for reduction strategies like CBN and CBV, which do
not allow reductions inside the bodies of $\lambda$-abstractions.

It would be nice if we could just say that two terms are equivalent if
they give equivalent results on equivalent inputs. Unfortunately, this
is a circular statement, so it doesn't define anything!  It is not
even clear that there is a ``right'' definition.

Another complication is undecidability. It is likely that any
reasonable notion of extensional equivalence will be undecidable due
to the relationship between the $\lambda$-calculus and Turing
machines. If we could test equivalence, then we could test equivalence
with $\Omega$, which is tantamount to solving the halting problem.

%\subsection{Observational Equivalence}

%
%Without loss of generality, we can simplify the definition to \[    
%e_1 = e_2 \ \ \iff\ \ \mbox{for all contexts } C[\hole],\ C[e_1]    
%\Downarrow \mbox{ iff } C[e_2] \Downarrow, \] because if they converge 
%to different values, it is possible to devise a context that causes one 
%to converge and the other to diverge. Suppose that $C[e_1] \Downarrow  
%v_1$ and $C[e_2] \Downarrow v_2$, where $v_1$ and $v_2$ have different 
%behavior. Then we can find some context $C'[\hole]$ which applies   
%to $v_1$ converges, and applied to $v_2$ diverges. Therefore, the  
%context $C'[C[\cdot]]$ is a context that causes the original $e_1$,
%$e_2$ to converge and diverge respectively, satisfying the simpler
%definition.

%
%A conservative approximation (but unfortunately still undecidable) is
%the following. Let $e_1$ and $e_2$ be terms, and suppose that $e_1$ and
%$e_2$ converge to the same value when reductions are applied according
%to some strategy. Then $e_1$ is equivalent to $e_2$.
%This _normalization_ approach (in which terms are reduced to a a _normal
%form_ on which no more reductions can be done) is useful for
%compiler optimization and for checking type equality in some advanced
%type systems.

\subsection{Contexts and Observational Equivalence}

Another approach to the problem of defining equivalence is to say that
two terms are equivalent if they behave indistinguishably in any
possible context.  But what do we mean by ``behave
indistinguishably''?

For simplicity, let us assume that we are working with an evaluation
strategy such as CBV or CBN that is _deterministic_, which means that
there is at most one next $\beta$-reduction that can be performed. We
say that a term $e$ _terminates_ or _converges_ if there is a finite
sequence of reductions
\[
e\ \to\ e'\ \to\ e''\ \to\ \cdots\ \to\ v
\]
leading to a value $v$. We write $e\Downarrow v$ when this happens,
and we write $e\Downarrow$ when $e\Downarrow v$ for some $v$. The
other possibility is that it keeps on reducing forever without ever
arriving at a value. When this happens, we say that $e$ _diverges_ and
write $e \Uparrow$. Because we have assumed that we are using a
deterministic evaluation strategy, exactly one of these two cases will
occur.

With CBN or CBV, there are infinitely many divergent terms. One
example is $\Omega$, which was defined in the last lecture.  We might
consider all divergent terms equivalent, since none of them produce a
value.

While we may not have a precise definition of extensional equivalence
yet, we can postulate a desirable property: two equivalent terms, when
placed in the same context, should either both diverge or both
converge and give indistinguishable values. Here a _context_ is any
term $C\hole$ with a single occurrence of a distinguished special
variable, called the _hole_, and $C\holed e$ denotes the context
$C\hole$ with the hole replaced by the term $e$. This notion of
equivalence is called \emph{observational equivalence}.

More formally, suppose we already have a notion of equivalence
$\equiv$ on values.  Then we will say that two terms
are \emph{observationally equivalent} (with respect to $\equiv$) and
write $e_1\eqobs e_2$ if for all contexts $C\hole$,
\begin{itemize}
\item
$C\holed{e_1} \Downarrow$ iff $C\holed{e_2} \Downarrow$; and
\item
if $C\holed{e_1} \Downarrow v_1$ and $C\holed{e_2} \Downarrow v_2$, then $v_1\equiv v_2$.
\end{itemize}
In other words, either both $C\holed{e_1}$ and $C\holed{e_2}$ diverge,
or both converge and produce equivalent values.

Note that on values themselves, equivalence is not necessarily the
same as observational equivalence. Certainly two values that are
observationally equivalent are equivalent in the sense of $\equiv$,
because we could put them in the trivial context consisting of just
the hole. However, the converse is not true: we could easily have
values that are equivalent in the sense of $\equiv$ but not
observationally equivalent. Is it possible to have $\eqobs$ and
$\equiv$ coincide on values? In other words, does there exist
a \emph{fixed point} of the transformation ${\equiv}\mapsto{\eqobs}$? If
so, is it unique? Even if not, is there a reasonable choice for the
definition of extensional equivalence?

The answers to these questions lie in the following facts, none of
which are difficult to prove.  We leave them as exercises.
\begin{lemma}
\label{lem:eqobs}
Let $\equiv$ be an arbitrary equivalence relation on values.
\begin{enumerate}
\renewcommand\labelenumi{\upshape(\roman{enumi})}
\item
The relation $\eqobs$ is an equivalence relation on terms.
\item
Restricted to values, $\eqobs$ \emph{refines} $\equiv$; that is,
viewed as sets of ordered pairs, $\eqobs$ restricted to values is a
subset of $\equiv$. Thus for any values $v_1$ and $v_2$, if $v_1\eqobs
v_2$, then $v_1\equiv v_2$.
\item
If $e_1\eqobs e_2$, then for all contexts $C\hole$,
$C\holed{e_1}\Downarrow$ iff $C\holed{e_2}\Downarrow$.
\item
The transformation ${\equiv}\mapsto{\eqobs}$ is \emph{monotone} with
respect to the refinement relation. That is, if $\equiv^1$ refines
$\equiv^2$, then $\equiv^1_{\mathrm{obs}}$ refines
$\equiv^2_{\mathrm{obs}}$.
\end{enumerate}
\end{lemma}
%\begin{proof}
%Miscellaneous Exercise \ref{}.
%\end{proof}
Now we can see that there are several fixed points of the transformation
${\equiv}\mapsto{\eqobs}$; the identity relation and the relation of
$\alpha$-equivalence, for two. This follows from
Lemma \ref{lem:eqobs}(i) and (ii). For CBV and CBN, there is also
a \emph{coarsest} one that is refined by every other fixed point: define
\begin{align*}
e_1\eqdown e_2\ \ &\Iffdef\ \ \text{for all contexts } C\hole,\ C\holed{e_1}\Downarrow \text{ iff } C\holed{e_2}\Downarrow.
\end{align*}
\begin{theorem}
\label{thm:obs}
For CBV and CBN, the relation $\eqdown$ is a fixed point of the transformation ${\equiv}\mapsto{\eqobs}$;
that is, ${\eqdown}=({\eqdown)_{\mathrm{obs}}}$. Moreover, it is the coarsest such fixed point.
\end{theorem}
%\begin{proof}
%Miscellaneous Exercise \ref{}.
%\end{proof}
The relation $\eqdown$ may be a reasonable candidate for extensional
equivalence.  By definition, to check that $e_1$ and $e_2$ are
observationally equivalent, it is enough to check that $e_1$ and $e_2$
both converge or both diverge in any context; it is unnecessary to
compare the resulting values in the case of convergence. This is
because if the values are not equivalent, one can devise a context in
which one converges and the other diverges.

%%%%%%%%%%%%%%%%
